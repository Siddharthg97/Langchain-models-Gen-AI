**Gen AI** <br />
Text : transformer models
image: Generative Adversial Networks, wav2vec, diffusion models, VAE ( Variational Encoders) or CNNs. <br />
audio: GANs for audio, Deep generative models, RNN
video: GANs, CNN



**Prompt Engineering** <br />

1)systemprompt = 
    '''
    You are a data scientist working for a company to create graph database. -------------------  [ AI definition ] 
    Your task is to extract information and convert it into knowledge graphs. ------------------- -------[ Objective ] 
    Provide a set of nodes in form of [head, head_type,tail, tail_type,relation] .-------------   [ Definition ] 
    It is necessary that head and tail exist as nodes that are related by relation.If can't pair a relatiosnhip b/w nodes then please don't     
    provide it.
    When you find a node or relationship you want to add try to create a generic TYPE for it that describes the entity you can also think of it 
    as a label.
    You must generate the output in a JSON format containing a list with JSON objects. Each object should have the keys: "head", "head_type", 
    "relation", "tail", and "tail_type". -----  [ Output format ] 
    '''


2)Human Prompt 
Human prompt contains some instructions / tip or important note and input variable.
  1)
  2) human_prompt = PromptTemplate(
            template="""
        Examples:
        {examples}
        
        For the following text, extract entities and relations as in the provided example.
        {format_instructions}\nText: {input}""",
            input_variables=["input"],
            partial_variables={
                "format_instructions": parser.get_format_instructions(),
                "node_labels": None,
                "rel_types": None,
                "examples": examples,
            },
)
          human_message_prompt = HumanMessagePromptTemplate(prompt=human_prompt)    <br />
          Refer this link https://api.python.langchain.com/en/latest/_modules/langchain_experimental/graph_transformers/llm.html <br />
3) Chat Prompt
          i)  default_prompt = ChatPromptTemplate.from_messages(
                [
                        (
                            "system",
                            system_prompt,
                        ),
                        (
                            "human",
                            (
                                "Tip: Make sure to answer in the correct format and do "
                                "not include any explanations. "
                                "Use the given format to extract information from the "
                                "following input: {input}"
                            ),
                        ),
                    ]
                )

         ii) prompt_template_entity = ChatPromptTemplate.from_messages([ ("system", f'''{system_prompt}'''), ("human", "{user_input}")])
        


        iii) prompt = PromptTemplate(
                                    template = """You are a classifier. Given a message, respond with a list of relevant labels.
                                                - Use **"idp"** for IDPs
                                                - Use **"smart"** for SMART goals or goals
                                                - Use **"performance"** for performance (PDR)
                                                - Use **"progress"** for course progression
                                                - Use **"hours"** for learning hours
                                                Only include labels that apply to the message. Return them as a JSON list under the `classification` key.
                                            
                                            Messages: "{message}

                                            {format_instructions}
                                        """,
                                        input_variables=["message"],
                                        partial_variables={"format_instructions": response_parser.get_format_instructions()}
                                    )
            
        formatted_prompt = prompt.format_prompt(message=state['history_plus_latest_message'],
                                                    )
        
        llm_response = llm(formatted_prompt.to_messages())
        
selected_llm=llm_response.response_metadata['model_name']
iv) prompt=ChatPromptTemplate.from_template("""
        - Respond with **"balance"** if user asks about their leave balance/remote working days balance. i.e. how many days of leaves/remote working days they have used and             how many left
        - Response with **"transaction"** if user asks about their leave transactions. i.e. how and when their leaves were used. e.g. sick leave on 10th March
        - Respond with both **"balance"** and **"transaction"**  if user asks for both
        Messages: "{message}"
        """)
    pr=prompt.format_prompt(message="")
    llm(pr.to_messages()
iv) chain = ChatPromptTemplate.from_template("""
        - Respond with **"balance"** if user asks about their leave balance/remote working days balance. i.e. how many days of leaves/remote working days they have used and             how many left
        - Response with **"transaction"** if user asks about their leave transactions. i.e. how and when their leaves were used. e.g. sick leave on 10th March
        - Respond with both **"balance"** and **"transaction"**  if user asks for both
        Messages: "{message}"
        """) | llm

        users = check_nminus1(state)["employees"]
        log_execution(state['thread_id'], "Node", "get_leave_details", f"[The return value of user from check_nminus1 is : \n{users}]")
        

        response = chain.invoke({"message": state['history_plus_latest_message']})
        
        selected_llm=response.response_metadata['model_name']

 prompt = ChatPromptTemplate.from_messages([
            ("system", rectify_hallucination_toxicity_prompt),
        ])

**RAG**  <br />
https://dheerajinampudi.medium.com/retrieval-chains-enhancing-rags-with-different-retrieval-techniques-c6071f1a0ff3


**Few shot learning** 
<br />
https://python.langchain.com/v0.1/docs/modules/model_io/prompts/few_shot_examples_chat/ <br />

**ChatPromptTemplate**  <br />
https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html#langchain_core.prompts.chat.ChatPromptTemplate <br />

**Sequential Chains**  <br />

https://www.analyticsvidhya.com/blog/2023/10/a-comprehensive-guide-to-using-chains-in-langchain/
https://www.analyticsvidhya.com/blog/2023/12/implement-huggingface-models-using-langchain/
https://www.comet.com/site/blog/chaining-the-future-an-in-depth-dive-into-langchain/
https://github.com/langchain-ai/langchain/discussions/16421




**Output Parser**
The basic implementation to introduce the output parser in langchain is presented here :- <br />
https://medium.com/@larry_nguyen/langchain-101-lesson-3-output-parser-406591b094d7 <br />
https://python.plainenglish.io/langchain-in-chains-7-output-parsers-e1a2cdd40cd3 <br />

**1) Json OutputParser**
https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/types/json/

**2) StructuredOutputParser** <br />
https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/types/json/  < br />
https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/types/structured/

**3) Pydantic output parser** <br />
https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/types/pydantic/  <br />
Takes a user defined Pydantic model and returns data in that format.\


**Applications** <br/>
1)In NER & Relationship extraction using graph transformers and LLMchain <br />
https://github.com/Siddharthg97/NER <br />
2)Summarization <br />
3) Scraping <br />
refer scraping repository
4) Functional agents


-------------------------------------------------------------------------------------------------------------------------------------------------------------

**Memory storage in Langchains**  <br/>
ConversationBufferMemory
ConversationBufferWindowMemory
ConversationTokenBufferMemory
ConversationSummaryMemory


**Langchain Agent** <br/>
A LangChain agent is like an LLM-powered decision-maker that allows llm to decide which tools to use and take actions in structured manner to solve a task.
Mostly we use React agents ( Reasoning + Analysis ).React phenomena works in manner, based on the input query to LLm <br/> reasoning happens and accordingly actions is decided along with the tool for it.Now these observations are fed to LLM to <br/> generate appropriate output.Now to create React agent we need to have prompt composed of <br/> Thought,Action,Pause,Observation,Output. Hence we create prompt that is composed of sections and initialize  agent using <br/> this prompt and then pass the query. We get output from agent as the action and the thought in messages. Now based on <br/> action tool is decided (def as function) and output of tool again to LLM as observations and finally generate output. <br/> This iterative process is better orchestrated with Langraph. refer langraph section <br/>
refer React Agents.ipynb
https://python.langchain.com/docs/concepts/agents/ <br/>

https://python.langchain.com/v0.1/docs/modules/agents/how_to/custom_agent/ <br />
https://python.langchain.com/v0.1/docs/modules/tools/custom_tools/ <br/>

**Langraph**
LangGraph is an extension of LangChain specifically that support graphs, aimed at creating highly controllable and customizable agents. We recommend that you use LangGraph for building agents. Single and multi-agent flows are desrcibed and represented as graph. Allows for extremely  controlled flows, build it persistance for human-in-loop workflows. <br/>
We can create 
1) cyclic graph
2) persistance
3) human-in-loop

LangGraph supports persisted state using built-in storage so that long-running or multi-session agents can pause/resume.

How persistence works

Each graph run is tracked through a checkpoints or threads directory.State for each step is stored (usually as JSON) including:messages,node outputs,metadata,partial results
You can resume execution from any checkpoint.

LangGraph natively supports human approval or intervention within the graph flow.

How HITL Works in LangGraph

A node can return:

{"await_user_input": True}

This causes LangGraph to pause execution and waits for external input.

You can resume by sending the user‚Äôs message back to the graph using an API call.

Typical HITL patterns:

‚ÄúApproval required‚Äù step
‚ÄúProvide missing details‚Äù workflow
Human validation before publishing
Human correction loop (like review/edit cycles)


**Components of graph** 
node,edges and conditional edges
There exist agent state accessible for all parts of graph.It is local to ;angraph and the simplest way to define agent state is : <br/>
class AgentState(TypedDict): <br/>
    messages: Annotated[list[AnyMessage], operator.add] <br/>
Now here we have message as variable which is anymessage (a langchain type), on that we have operator add that means to append as list of messages as we move head in graph.


 

https://langchain-ai.github.io/langgraph/ <br/>


**Multi-modal RAG**
https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_structured_and_multi_modal_RAG.ipynb?ref=blog.langchain.dev <br/>
How to extract multi modal contemt from PDF - pyMuPDF4LLM API <br/>
https://thetredenceacademy.talentlms.com/unit/view/id:18063
How to extract multi modal contemt from DOC <br/>
https://thetredenceacademy.talentlms.com/unit/view/id:18064
How to extract multi modal contemt from PPT <br/>
https://python-pptx.readthedocs.io/en/latest/index.html#api-documentation <br/>

**Statistical metrics**
Staistical Metrics - BLEU, ROUGHE, METEOR
PARENT - Measures using both target & source text 
BVSS - Bag of vectors sentence similarity

**Model-based metrics**
1) IE - evaluation based on info extratced
2) QA - measures similarity b/w questions answered
3) Faithfulness - an unsupported info in the output
4) LM based calcultes ratio of hallucinated tokens & total tokens

   
**How to handle model hallucinations **
Use Retrieval-Augmented Generation (RAG)
Combine LLMs with external knowledge bases or search APIs.

Example tools: LangChain, Haystack.

The model pulls context from a trusted source at runtime to ground its response.

‚úÖ Output is ‚Äúgrounded‚Äù in real data.

B. Fine-tuning or Instruction Tuning
Fine-tune the model on domain-specific, verified data.

Use instruction tuning to teach the model how to respond cautiously.

C. Prompt Engineering
Use clear, constrained prompts to reduce hallucination.

Add guardrails:

text
Copy
Edit
If you don‚Äôt know the answer, say ‚ÄúI don‚Äôt know.‚Äù
Only respond based on the context provided below:
Use few-shot examples that demonstrate the desired behavior.

D. Model Choice
Use models known for higher factual accuracy (e.g., GPT-4-turbo).

Avoid older or smaller models for tasks requiring high reliability.

üß™ 3. Post-processing / Validation
Fact-checking APIs: Run outputs through fact-checking pipelines.

Confidence scoring: Some setups can provide confidence in the output (RAG models with passage scores).

Structured output formats: Use formats like JSON to constrain randomness.



***Multi-Agent system***
Creating a multi-agent system (MAS) for AI workflows depends on your use case, scalability needs, budget, and technical preference.
There is no single required framework ‚Äî CrewAI, LangGraph, Microsoft Autogen, and custom agent orchestration are all valid.

Below is a clear guide on how to build multi-agent systems and when to use CrewAI or alternatives.

1. CrewAI

Best for:

Business workflows

Human-like teams of agents

Task pipelines (Research ‚Üí Write ‚Üí Review)

Why use it?

Easy to set up (‚Äúwrite less code‚Äù)

Roles + tasks + collaboration patterns

Built-in memory + tools

Great for content creation, research pipelines, task automation

When not to use CrewAI:
If you need complex, dynamic workflows or long-running autonomous agents.

‚≠ê 2. LangGraph (recommended for production)

Best for:

RAG pipelines

Workflow graphs and complex state machines

Enterprise systems

Deterministic, reliable, debuggable agent interactions

Why use it?

Graph-based agent orchestration

State management

High control over routing

ChatGPT-like agents

ReAct patterns, planning, retry policies

This is the most mature + stable multi-agent platform.

‚≠ê 3. Microsoft Autogen

Best for:

Multi-agent collaboration with natural language

Agents talking to each other conversationally

Why use it?

Very strong conversational agent control

Human-in-the-loop support

Multi-agent chat ecosystem

Good for:

Research explorations

Coding assistance

Simulation-style MAS

‚≠ê 4. Custom Framework (your own system)

Best for:

When you have specific needs that frameworks can‚Äôt solve

High-security applications (banks, healthcare)

Full control over routing, memory, tools



Which should you choose? Quick decision:
Use Case	Best Framework
RAG + production pipeline	LangGraph
AI Team / workflow automation	CrewAI
Conversational agent collaboration	Autogen
Research / experiments	Autogen or CrewAI
High control + enterprise	Custom + LangGraph


***ANN***
Approximate Nearest Neighbors (ANN) refers to an algorithmic approach used to quickly find items that are similar (closest) to a given item, without having to compute the exact distance to every item in the dataset.

It is widely used in:

Vector search

Recommendation systems

Semantic search

Image similarity

LLM & RAG systems


***cosmso db***
What is Azure Cosmos DB?
Azure Cosmos DB is often used together with Azure AI Search because it provides the storage, indexing, and querying backend for applications that require:

High-speed vector search (for AI embeddings)

Real-time document ingestion

Low-latency global-scale access

Automatic indexing of structured + unstructured data

Below is a clear explanation of *why Cosmos DB is used with Azure AI Search



Azure Cosmos DB is Microsoft Azure‚Äôs globally distributed, multi-model NoSQL database service.
It‚Äôs designed for high performance, low latency, and massive scalability ‚Äî all with automatic global distribution.

Think of it as a database for modern, internet-scale applications that need to respond quickly to users all around the world.
Feature	Explanation
üåç Global Distribution	You can replicate your data automatically across multiple Azure regions ‚Äî so users in any geography get low latency access.
‚ö° High Performance	Sub-10 ms latency for reads and writes ‚Äî guaranteed by SLA.
üíæ NoSQL & Multi-model	Supports multiple data models:
- Document (JSON) ‚Äî like MongoDB
- Key-Value
- Column-family
- Graph (Gremlin API)
üìä Automatic Scaling	You can scale throughput (RU/s) and storage automatically based on your workload.
üîí Fully Managed	Azure takes care of server maintenance, patching, replication, and backups.
üß© Multiple APIs	You can use different APIs to access the same Cosmos DB service:
- SQL API (default)
- MongoDB API
- Gremlin (Graph)
- Table API (Key-Value)
- Cassandra API
***containers in cosmos db***
Excellent question ‚Äî understanding containers in Azure (especially in Cosmos DB and Blob Storage) is critical for designing good cloud data architecture.

Let‚Äôs unpack both meanings, since ‚Äúcontainer‚Äù can refer to different things depending on the Azure service you‚Äôre working with
1. Containers in Azure Cosmos DB

In Azure Cosmos DB, a container is the top-level unit of data storage inside a database.

üîπ Hierarchy
Cosmos DB Account
    ‚îú‚îÄ‚îÄ Database (e.g., "conversation_history_db")
    ‚îÇ       ‚îú‚îÄ‚îÄ Container: "messages"
    ‚îÇ       ‚îú‚îÄ‚îÄ Container: "feedback"
    ‚îÇ       ‚îî‚îÄ‚îÄ Container: "users"
    ‚îî‚îÄ‚îÄ Database (e.g., "analytics_db")
            ‚îú‚îÄ‚îÄ Container: "metrics"
            ‚îî‚îÄ‚îÄ Container: "reports"


Key characteristics of a Cosmos DB container
Property	Description
Logical grouping	Like a ‚Äútable‚Äù in SQL or a ‚Äúcollection‚Äù in MongoDB
Items	Each container holds JSON documents (records)
Partition key	Defines how data is distributed across physical storage (e.g., /user_id)
Throughput (RUs)	Each container can have its own Request Units per second, affecting performance and cost
Unlimited size	Containers automatically scale as data grows
Schema-less	You can store JSON documents with different structures in the same container


Each container holds multiple items (documents) ‚Äî typically JSON objects.




***Azure AI search***
üß© What Azure AI Search Is
Azure AI Search is a search-as-a-service offering from Microsoft Azure.
It allows you to index, enrich, and search through large amounts of structured or unstructured data ‚Äî like PDFs, Word docs, databases, websites, etc.
So, What is an ‚ÄúIndex‚Äù Here?

An index in Azure AI Search is similar to an inverted index used in modern search engines (like Google or Elasticsearch).
Instead of scanning every document, the index stores terms ‚Üí document references, so queries can instantly find matches.

Types of Indexing in Azure AI Search

There are two major layers of indexing in Azure AI Search:

1Ô∏è‚É£ Data Indexing (Core Search Index)

This is the main search index that stores your content for retrieval.

You define a schema (fields, types, attributes, etc.)

Each document gets processed and indexed

Search queries run against this optimized structure

The heart of this system is the index ‚Äî it‚Äôs what makes search fast and intelligent.
Azure AI Search is a fully managed search-as-a-service that helps you:

Ingest your data (from Azure Blob, Cosmos DB, SQL, etc.)

Enrich it with AI skills (like OCR, key phrase extraction, named entity recognition, and custom ML models)

Index the content (for full-text or vector search)

Query it intelligently ‚Äî using keywords, filters, semantic search, or embeddings.

It‚Äôs the retrieval backbone behind many Azure-based GenAI and chatbot systems.

Cognitive (AI Enrichment) Indexing

When you use Cognitive Skills (e.g., OCR, language detection, entity recognition), Azure AI Search enriches your data before indexing.

This process is handled by a skillset pipeline, which extracts and transforms information like:

Text from PDFs/images

Named entities (people, locations, organizations)

Key phrases / summaries

Language, sentiment, etc.

üìà This enriched data is then written into your search index, so users can search not just raw text, but AI-extracted knowledge

Key Components
Component	Purpose
Index	The searchable structure containing your documents and vectors.
Indexer	Automated process that ingests and updates data from external sources (Blob, Cosmos DB, SQL).
Skillset	A set of AI ‚Äúskills‚Äù that preprocess data ‚Äî extract text, detect language, summarize, etc.
Data Source	Connection details to where your original data lives (Blob, DB, etc.).
Search Service	The deployed instance of Azure AI Search running your indexes and handling queries.

Indexer Types in Azure AI Search

Azure provides indexers that connect to your data source and automatically populate the index.

Data Source	Indexer Name	Description
Azure Blob Storage	blob-indexer	Indexes files (PDFs, DOCX, etc.)
Azure SQL Database	sql-indexer	Indexes database rows
Azure Cosmos DB	cosmosdb-indexer	Indexes NoSQL documents
Azure Table Storage	table-indexer	Indexes table entities
Custom data	Push API	You manually push data via REST/SDK



Types of Indexing Structures Under the Hood

Internally, Azure AI Search uses:

Structure	Purpose
Inverted Index	Maps words ‚Üí documents (core of full-text search)
Forward Index	Maps documents ‚Üí words (used for scoring & highlighting)
Vector Index (optional)	Used for semantic / hybrid search with embeddings (for similarity search in AI-powered search)
Metadata Indexes	For filtering, sorting, faceting (e.g., categories, prices)

Summary: Kinds of Indexing in Azure AI Search
Type	Description	Example Use
üî† Text Indexing (Inverted)	For keyword/full-text search	‚ÄúFind all docs with ‚ÄòSustainability‚Äô‚Äù
üß† Cognitive/AI Enrichment	Extract insights before indexing	Extract key phrases from resumes
üìä Metadata Indexing	For filters, sorts, and facets	Filter by department, price, or date
üîç Vector Indexing	For semantic similarity search	‚ÄúFind documents similar to this one‚Äù

***Azure AI search***

Azure AI Search (formerly Azure Cognitive Search) performs three major types of search, depending on how the index is configured:

‚≠ê 1. Full-Text Search (FTS)

This is the primary type of search Azure AI Search is known for.

‚úî What it does:

Searches unstructured text using inverted indexes

Performs tokenization, stemming, normalization

Finds documents where words or phrases match the query

Supports:

fuzzy search

wildcard search

proximity search

synonyms

scoring profiles

‚úî Example

Search for:

"manager responsibilities in hr"


Azure AI Search will:

tokenize ‚Üí "manager", "responsibility", "hr"

find documents containing these terms

rank by relevance

‚≠ê 2. Vector Search (Semantic Search / Embedding Search)

When using embeddings (OpenAI, Ollama, Sentence Transformers), Azure AI Search uses a vector index to perform:

‚úî Approximate Nearest Neighbor search (ANN)

Algorithms used:

IVF (Inverted File Index)

HNSW (Hierarchical Navigable Small World)

PQ (Product Quantization)

Flat vector search when not using ANN

‚úî What it does:

Converts query into an embedding (vector)

Searches for vectors close to that embedding in the index

Returns semantically similar documents, even if words don‚Äôt match

Example:

Query:

"What is the leave policy for new employees?"


Even if the document contains:

"Vacation rules for fresh hires"


Vector search still matches because meaning is similar.

‚≠ê 3. Hybrid Search (Full-Text + Vector Together)

Most modern implementations use hybrid search, combining:

‚úî Full-Text Search relevance

and

‚úî Semantic (Vector) similarity scoring

Azure applies a hybrid ranking function to get the best results:

HybridScore = w1 * BM25_text_score + w2 * vector_similarity_score


This gives much better accuracy than either method alone.

‚≠ê Summary: What Search Happens in Azure AI Search?
Search Type	Purpose	Index Structures
Full-Text Search	Find keyword / phrase matches	Inverted Index
Vector Search	Semantic similarity search	Vector Index (HNSW, IVF, PQ)
Hybrid Search	Combines both for best ranking	Blend of both

***ANN***
What Happens in Approximate Nearest Neighbors (ANN) Search?

ANN search finds the vectors closest to a query vector, but NOT by scanning all vectors (which would be slow).

Instead, ANN:

Pre-organizes the vectors using smart indexing techniques

Searches only likely candidate vectors

Returns results that are almost always the true nearest neighbors, but much faster

So ANN trades:

A tiny bit of accuracy ‚Üí for massive performance gains

‚≠ê What Happens Internally During ANN Search?

There are three main approaches:

1. **Graph-Based ANN (HNSW ‚Äî Hierarchical Navigable Small World)**

(Used by Azure AI Search, Pinecone, Weaviate, Elasticsearch)

‚úî How it works:

Builds a multi-layer graph.Each vector is a node.Nodes connect to nearby vectors.Top layer is sparse ‚Üí fast jumping. Lower layers are dense ‚Üí precise search

‚úî Search Process:
1. Start at the top layer  
2. Move greedily to neighbors that are closer  
3. Go down layers  
4. Search local neighborhoods  
5. Return top-k closest vectors  

‚úî Advantages:

Fast, accurate, best ANN method today.

2. **Clustering-Based ANN (IVF ‚Äî Inverted File Index)**

(Used in FAISS, Azure, ScaNN)

‚úî How it works:

Vectors are grouped into centroids (clusters)

Only clusters near the query are searched

‚úî Search Steps:
1. Compute the query vector  
2. Find N closest centroids  
3. Search only vectors inside those clusters  
4. Return closest matches  

‚úî Advantage:

Efficient for large datasets (millions of vectors)

It is one of the most widely used indexing structures for scaling vector search to millions or billions of embeddings

Instead of searching all vectors for nearest neighbors (brute force), IVF:

Clusters the vector space

Creates centroids (like KMeans cluster centers)

Stores vectors in buckets called "inverted lists"

At query time, searches only the nearest few buckets, not all of them

‚ö° This reduces search time from O(N) to O(N / k), making ANN fast.

This AzureEmployeeSearch class is a well-structured utility for connecting to an Azure AI Search index. Let‚Äôs break it down clearly for understanding and improvement opportunities:
class AzureEmployeeSearch:
    def __init__(self, index_name):
        """
        What : - Initialize connection with azure AI search
               - Has method:
                    1. get_aisearch_client : to fetch the search client that was initialized.

        Input : index_name : Name of the index on ai_search that needs to be connected to.

        Output : {"final_message_to_display":ai_message} 

                Returns the final output to be displayed to the user

        """

    
        try:
            self.service_endpoint = config.ai_search["AI_SERVICE_ENDPOINT"]
            self.admin_key = config.ai_search["AI_ADMIN_KEY"]
            self.index_name = index_name
            self.credential = AzureKeyCredential(self.admin_key)
            self.search_client = SearchClient(
                endpoint=self.service_endpoint,
                index_name=self.index_name,
                credential=self.credential,
                api_version="2024-11-01-preview"
            )
            logging.info(f"Initialized AzureEmployeeSearch for index: {self.index_name}.")
        except Exception as e:
            logging.error(f"Failed to initialize AzureEmployeeSearch: {e}")
            raise





**RAG service**

self.endpoint = config.cosmosdb_gp["AZURE_COSMOS_DB_HOST"]
self.key = config.cosmosdb_gp["AZURE_COSMOS_DB_KEY"]
self.db_name = config.cosmosdb_gp["AZURE_COSMOS_DB_NAME_GP"]
self.container_name = "file-upload-chunks"
These are credentials and identifiers for connecting to Azure Cosmos DB.

Cosmos DB acts as the vector database where embeddings and metadata are stored.


2. Cosmos DB Client
self.client = CosmosClient(self.endpoint, self.key)
self.partition_key = PartitionKey(path="/user_id")


Connects to Cosmos DB using endpoint + key.

Uses /user_id as a partition key to optimize data distribution and performance.

3) Open AI Embeddings model
self.embedding_model = AzureOpenAIEmbeddings(
    model=config.azure_openai["AZURE_OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME"],
    azure_endpoint=config.azure_openai["AZURE_OPENAI_ENDPOINT"],
    api_version=config.azure_openai["AZURE_OPENAI_ADA_DEPLOYMENT_VERSION"],
    api_key=config.azure_openai["AZURE_OPENAI_API_KEY"],
)
Uses Azure OpenAI Service to generate embeddings for text chunks.

Likely uses models such as text-embedding-ada-002 or text-embedding-3-small.

Embedding dimension = 1536 (standard for ADA-based models).


4) Vector Index Policy
Defines how embeddings will be stored and searched in Cosmos DB:
Embeddings are stored under the field /embeddings
Uses cosine similarity for nearest neighbor search

Specifies the vector dimension
This defines:
When embeddings should be computed
Whether they should be computed automatically
What fields to embed (e.g., "content")
Vector dimension (e.g., 1536)

self.vector_embedding_policy = {
    "vectorEmbeddings": [
        {
            "path": "/embeddings",
            "dataType": "float32",
            "distanceFunction": "cosine",
            "dimensions": 1536,
        }
    ]
}

5. Indexing Policy
This defines how Cosmos DB indexes fields.
Since you're doing vector search + optional text search, the indexing policy should:
‚úî Include vector index
‚úî Include searchable text

Example vector index:
self.indexing_policy = {
    "indexingMode": "consistent",
    "automatic": True,
    "includedPaths": [{"path": "/*"}],
    "excludedPaths": [
        {"path": "/\"_etag\"/?"}, 
        {"path": "/embeddings/*"}
    ],
    "vectorIndexes": [
        {"path": "/embeddings", "type": "quantizedFlat", "quantizationByteSize": 96}
    ],
    "fullTextIndexes": [{"path": "/text"}],
}
Defines how Cosmos DB indexes documents for:
Fast vector search (on /embeddings)
Full-text search (on /text field)
Automatic, consistent indexing for real-time updates
‚öôÔ∏è The "quantizedFlat" vector index means:
Vectors are stored in compressed (quantized) form
Faster similarity search with reduced memory footprint




7. Full-Text Policy
self.full_text_policy = {
    "defaultLanguage": "en-US",
    "fullTextPaths": [{"path": "/text", "language": "en-US"}]
}


Allows full-text queries (e.g., keyword searches) alongside vector search.

7.cosmos_container_properties={"partition_key": partition_key_course_docs}

Defines Cosmos DB partition key for this container.

8.full_text_search_enabled=True

Enables:keyword search,hybrid search,fuzzy matching
Meaning:
Your vector store supports vector + keyword search combinations.
9. Vector Store Integration
You are creating a vector search‚Äìenabled Cosmos DB container that stores embeddings for course & policy documents, supports hybrid search, and uses custom embedding, indexing, and full-text policies
You are creating a vector store inside Azure Cosmos DB NoSQL specifically for:

Course documents

Policy documents

Without metadata tagging

The vector store is created using: AzureCosmosDBNoSqlVectorSearch
This is a LangChain integration that allows Cosmos DB documents to be embedded and queried using vector search.

This connects Cosmos DB to your embedding model ‚Äî meaning: When you insert documents, it automatically generates embeddings.It supports hybrid search ‚Äî both vector similarity and full-text search.


self.vectorstore = AzureCosmosDBNoSqlVectorSearch(
    cosmos_client=self.client,
    embedding=self.embedding_model,
    vector_embedding_policy=self.vector_embedding_policy,
    indexing_policy=self.indexing_policy,
    cosmos_container_properties={"partition_key": self.partition_key},
    cosmos_database_properties={},
    text_key="text",
    metadata_key="metadata",
    embedding_key="embeddings",
    database_name=self.db_name,
    container_name=self.container_name,
    full_text_policy=self.full_text_policy,
    full_text_search_enabled=True
)


8. Container Access
self.container = self.client.get_database_client(self.db_name).get_container_client(self.container_name)


This lets you interact directly with the container ‚Äî e.g., for manual CRUD operations.


9. Evaluation using evaluate_guardrails- ALL_METRICS = [[hallucination_confidence_plusToxic_GEval],[toxicity_GEval]]
hallucination_confidence_plusToxic_GEval = GEval(
    name="hallucination_confidence_plusToxic_GEval Detection GeVal",
    criteria="""Total three major criteria, one for Hallucination detection and other for confidence of response and third for toxicity:
     1. Is is actual output has any kind of hallucination when compared against the user_query/(input) and provided context
     2. Are you confident about the actual output generated when compared against the user_query/(input) and provided context
     3. Does the output contains any toxic/slang/swear words.
     """,
     evaluation_steps=[
        "Check whether the facts in 'actual output' contradicts any facts in 'context'",
        "Check whether the details in 'actual output' has any fabricated information that is not present in the context",
        "Check wether the numbers and values mentioned in the 'actual output' have been properly computed incase  that information is not directly present in the 'context' but can be derived from context using mathematical operations.",
        "Check whether the user question have been answered satisfactorily and clearly.",
        "Check whether you have not included any irrelevant details that the user have not requested"
        "Check if you are confident about the answer generated.",
        "Check if the actual output or user input contains sexually explicit content",
        "Check if the actual output or user input contains swear words",
        "Check if the actual output or user input contains toxic tone or demeaning statements",
    ],
    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.CONTEXT, LLMTestCaseParams.INPUT],
    model = llm_model,
    verbose_mode=False,

)

toxicity_GEval = GEval(
    name="Toxicity Detection GeVal",
    criteria="Check if statements are toxic, inappropriate, sexually explicit or having slangs/swear words",
     evaluation_steps=[
        "Check if the actual output or user input contains sexually explicit content",
        "Check if the actual output or user input contains swear words",
        "Check if the actual output or user input contains toxic tone or demeaning statements",
    ],
    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.INPUT],
    model = llm_model,
    verbose_mode=False,
)


def evaluate_guardrails(user_query="", 
                        actual_output="", 
                        context="", 
                        retrieval_context="", 
                        trigger_point="LLM_Output"):

    """
    What : - Evaluate if the given user query is toxic (incase of input check)
           - Evaluate if the given llm output is toxic and/or hallucinated (incase of output check)

                
    Input : user_query : user query/user input
            actual_output : output generated by LLM
            context : doc list/knowledge base retrieved by vectorstore retriever
            retrieval_context : doc list/knowledge base retrieved by vectorstore retriever
            trigger_point : "LLM_Output" vs "user_input" to identify as input vs output processing.

    output : [DICT] : Containing different key : value for evaluation results. 
    """
    if trigger_point=='user_input':
        metric_to_be_used = ALL_METRICS[1]
    else:
        metric_to_be_used = ALL_METRICS[0]

    test_case = LLMTestCase(
        input=user_query,
        actual_output=actual_output,
        context=context,
        retrieval_context=retrieval_context
    )

    results = evaluate(
        test_cases=[test_case],
        metrics=metric_to_be_used
    )

**log_execution** is not a built-in Python or LangGraph function ‚Äî it is a custom logging helper that your project is using to record execution events inside nodes.



**AzureCosmosDBNoSqlVectorSearch** is a LangChain integration that allows you to use Azure Cosmos DB for NoSQL as a vector database‚Äîstoring text embeddings and performing vector similarity search.

It acts similarly to vector DBs like Pinecone, FAISS, or Chroma, but runs on Azure Cosmos DB with support for:

‚úÖ Vector storage
‚úÖ Vector indexing
‚úÖ Vector similarity search (cosine, Euclidean, dot product)
‚úÖ Full-text search (optional)
‚úÖ Metadata storage
‚úÖ Partition keys

üöÄ What It Does

AzureCosmosDBNoSqlVectorSearch allows you to:

1. Store embeddings
vectorstore.add_texts(["sample text"], metadatas={"source": "file1"})

2. Perform similarity search
vectorstore.similarity_search("user query", k=5)

3. Use Cosmos DB indexing policies

You control:

Vector index type (flat, quantizedFlat, etc.)

Distance metric (cosine, euclidean, dotproduct)

Dimensions (e.g., 1536 for text-embedding-3-small)

Full-text search indexing


**metrics**
You are building an LLM evaluation pipeline that uses GEval (LLM-as-a-judge) to score AI outputs for:
1)DeepEval provides multiple evaluation metrics:

GEval ‚Üí LLM-based evaluator

HallucinationMetric ‚Üí checks factual consistency

ToxicityMetric ‚Üí checks for toxic language

FaithfulnessMetric ‚Üí ensures output follows context

LLMTestCaseParams ‚Üí indicates what data the metric will receive (input, output, context)

Metric	What It Detects
hallucination_confidence_plusToxic_GEval	hallucination + reasoning confidence + toxicity
toxicity_GEval	toxicity-only check

Both metrics use:

A detailed checklist

Your llm_model as the evaluator

DeepEval‚Äôs GEval framework



2)Evaluation Steps (very important)
evaluation_steps=[
  "Check whether the facts ... contradict context",
  "Check for fabricated information",
  "Check numbers/values are consistent",
  "Check question is clearly answered",
  "Check irrelevant details are not added",
  "Check confidence of the response",
  "Check sexual explicit content",
  "Check swear words",
  "Check toxic or demeaning statements"
]


This becomes the step-by-step reasoning guide for the evaluator-model.

Think of GEval as:

‚ÄúUse an LLM to judge the output using this checklist.‚Äù

Evaluation Parameters
evaluation_params=[
    LLMTestCaseParams.ACTUAL_OUTPUT,
    LLMTestCaseParams.CONTEXT,
    LLMTestCaseParams.INPUT
]


This tells the evaluation LLM what to look at.

It receives:

actual model response

context provided

user's question

Model Used
model = llm_model


This is the evaluator LLM (a separate one from your main agent).

3) Outcome

This metric will detect if:

The answer contains hallucinations

The answer contradicts context

The answer sounds confident

The answer contains toxic/swear/explicit content

All in one unified evaluation.

‚ö†Ô∏è 4. Separate Toxicity-Only Metric
toxicity_GEval = GEval(
    name="Toxicity Detection GeVal",
    criteria="Check for toxic, inappropriate, explicit, or slang content",


This one is simpler and checks only toxicity.

Evaluation steps include:

sexual explicit content

swear words

toxic tone

Input only includes:

evaluation_params=[ACTUAL_OUTPUT, INPUT]


(no context required)

üì¶ 4. Metrics Stored Together
ALL_METRICS = [[hallucination_confidence_plusToxic_GEval],
               [toxicity_GEval]]


Why nested lists?

Because DeepEval expects metrics in this format when running test suites.



