{"cells":[{"cell_type":"code","execution_count":1,"id":"0_1750603488376","metadata":{},"outputs":[],"source":["# !pip install langchain faiss-cpu pdfplumber scikit-learn transformers sentence-transformers accelerate bitsandbytes\r\n","# !pip install \"pydantic>=2.0\"\r\n","# !pip install -U langchain-community\n"]},{"cell_type":"code","execution_count":1,"id":"1_1750603488376","metadata":{},"outputs":[],"source":["# !pip install numpy==1.24.4 scipy==1.10.1 scikit-learn==1.2.2\n"]},{"cell_type":"code","execution_count":1,"id":"2_1750603488376","metadata":{},"outputs":[],"source":["#%restart_python\n"]},{"cell_type":"code","execution_count":1,"id":"3_1750603488376","metadata":{},"outputs":[],"source":["import os\r\n","import pdfplumber\n"]},{"cell_type":"code","execution_count":1,"id":"4_1750603488376","metadata":{},"outputs":[],"source":["import requests\r\n","\r\n","# URLs of the annual reports\r\n","pdf_urls = {\r\n","    \"HPS_Annual_Report_2022.pdf\": \"https://bytesizecomposableaisa.blob.core.windows.net/gen-ai-assignment/HPS%20Annual%20Report%202022.pdf?sv=2025-01-05&st=2025-06-22T08%3A17%3A06Z&se=2025-09-20T08%3A17%3A06Z&sr=b&sp=r&sig=CfGcnbLmN6v%2BjIdvDtVl2guq4kSqcUn2QIkdeQ3Wir0%3D\",\r\n","    \"Hubble_Annual_Report.pdf\": \"https://bytesizecomposableaisa.blob.core.windows.net/gen-ai-assignment/Hubble%20Annual%20Report.pdf?sv=2025-01-05&st=2025-06-22T08%3A17%3A06Z&se=2025-09-20T08%3A17%3A06Z&sr=b&sp=r&sig=cQVYGjqGeIz2opXLGF%2FzYdCchZiQa%2FdqzdVVBES363o%3D\",\r\n","    \"Schneider_Electric_Annual_Report.pdf\": \"https://bytesizecomposableaisa.blob.core.windows.net/gen-ai-assignment/Schneider%20Electric%20Annual%20Report.pdf?sv=2025-01-05&st=2025-06-22T08%3A17%3A06Z&se=2025-09-20T08%3A17%3A06Z&sr=b&sp=r&sig=N6byFRCtk4TdzokyfCaBKuzGJEuPU8af8pLECXvu3IE%3D\"\r\n","}\r\n","\r\n","# Download and save each PDF\r\n","for filename, url in pdf_urls.items():\r\n","    response = requests.get(url)\r\n","    if response.status_code == 200:\r\n","        with open(filename, \"wb\") as f:\r\n","            f.write(response.content)\r\n","        print(f\"Downloaded and saved: {filename}\")\r\n","    else:\r\n","        print(f\"Failed to download: {filename}\")\n"]},{"cell_type":"code","execution_count":1,"id":"5_1750603488376","metadata":{},"outputs":[],"source":["# === Step 1: Load PDF Text ===\r\n","def load_pdf_text(file_path):\r\n","    all_text = \"\"\r\n","    with pdfplumber.open(file_path) as pdf:\r\n","        for page in pdf.pages:\r\n","            all_text += page.extract_text() or \"\"\r\n","    return all_text\r\n","\r\n","# === Step 2: Define PDF Files ===\r\n","pdf_files = {\r\n","    \"HPS\": \"HPS_Annual_Report_2022.pdf\",\r\n","    \"Hubble\": \"Hubble_Annual_Report.pdf\",\r\n","    \"Schneider\": \"Schneider_Electric_Annual_Report.pdf\"\r\n","}\n"]},{"cell_type":"code","execution_count":1,"id":"6_1750603488376","metadata":{},"outputs":[],"source":["hf_token = \"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\"\n"]},{"cell_type":"code","execution_count":1,"id":"7_1750603488376","metadata":{},"outputs":[],"source":["import os\r\n","import pdfplumber\r\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n","from langchain.docstore.document import Document\r\n","from langchain.vectorstores import FAISS\r\n","from langchain.chains import RetrievalQA\r\n","from langchain.embeddings import HuggingFaceEmbeddings\r\n","from langchain.llms import HuggingFacePipeline\r\n","from transformers import pipeline\r\n","from huggingface_hub import login\r\n","\r\n","# === STEP 1: Load PDF Text ===\r\n","def load_pdf_text(file_path):\r\n","    text = \"\"\r\n","    with pdfplumber.open(file_path) as pdf:\r\n","        for page in pdf.pages:\r\n","            page_text = page.extract_text()\r\n","            if page_text:\r\n","                text += page_text + \"\\n\"\r\n","    return text\r\n","\r\n","# === STEP 2: Split Text into Chunks ===\r\n","def split_text(text, chunk_size=1000, chunk_overlap=200):\r\n","    splitter = RecursiveCharacterTextSplitter(\r\n","        chunk_size=chunk_size,\r\n","        chunk_overlap=chunk_overlap\r\n","    )\r\n","    return splitter.split_text(text)\r\n","\r\n","# === STEP 3: Convert to LangChain Documents ===\r\n","def prepare_documents(pdf_files):\r\n","    documents = []\r\n","    for company, file_path in pdf_files.items():\r\n","        print(f\"üìÑ Processing {company}\")\r\n","        text = load_pdf_text(file_path)\r\n","        chunks = split_text(text)\r\n","        docs = [Document(page_content=chunk, metadata={\"company\": company}) for chunk in chunks]\r\n","        documents.extend(docs)\r\n","    return documents\r\n","\r\n","# === STEP 4: Load Embeddings ===\r\n","def load_embeddings():\r\n","    return HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\r\n","\r\n","# === STEP 5: Load LLM (Optional Token) ===\r\n","def load_local_llm(model_name=\"tiiuae/falcon-7b-instruct\", hf_token=None):\r\n","    if hf_token:\r\n","        login(hf_token)  # Authenticate if token is provided\r\n","\r\n","    pipe = pipeline(\r\n","        \"text-generation\",\r\n","        model=model_name,\r\n","        tokenizer=model_name,\r\n","        max_new_tokens=512,\r\n","        temperature=0.3,\r\n","        top_p=0.95,\r\n","        do_sample=True,\r\n","        device=-1  # Use 0 for GPU\r\n","    )\r\n","    return HuggingFacePipeline(pipeline=pipe)\r\n","\r\n","# === STEP 6: Build Vectorstore ===\r\n","def build_vectorstore(docs, persist_path=\"faiss_index\"):\r\n","    embeddings = load_embeddings()\r\n","    vectorstore = FAISS.from_documents(docs, embeddings)\r\n","    vectorstore.save_local(persist_path)\r\n","    return vectorstore\r\n","\r\n","# === STEP 7: Load QA Chain ===\r\n","def build_qa_chain(vectorstore, model_name=\"tiiuae/falcon-7b-instruct\", hf_token=None):\r\n","    llm = load_local_llm(model_name=model_name, hf_token=hf_token)\r\n","    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\r\n","    return RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\r\n","\r\n","# === MAIN ===\r\n","if __name__ == \"__main__\":\r\n","    pdf_files = {\r\n","        \"HPS\": \"HPS_Annual_Report_2022.pdf\",\r\n","        \"Hubble\": \"Hubble_Annual_Report.pdf\",\r\n","        \"Schneider\": \"Schneider_Electric_Annual_Report.pdf\"\r\n","    }\r\n","\r\n","    # Path to persist vectorstore\r\n","    persist_path = \"faiss_index\"\r\n","\r\n","    # Optional: Hugging Face token if using gated/private models\r\n","    hf_token = os.getenv(\"HF_TOKEN\")  # or paste it directly for testing\r\n","\r\n","    # Step 1‚Äì3: Load and chunk documents\r\n","    if not os.path.exists(persist_path):\r\n","        print(\"üìö Building vectorstore from documents...\")\r\n","        documents = prepare_documents(pdf_files)\r\n","        vectorstore = build_vectorstore(documents, persist_path)\r\n","    else:\r\n","        print(\"üì¶ Loading existing vectorstore...\")\r\n","        embeddings = load_embeddings()\r\n","        vectorstore = FAISS.load_local(persist_path, embeddings)\r\n","\r\n","    # Step 7: QA Chain (replace model name if needed)\r\n","    qa_chain = build_qa_chain(vectorstore, model_name=\"tiiuae/falcon-7b-instruct\", hf_token=hf_token)\r\n","\r\n","    # Step 8: Ask questions\r\n","    questions = [\r\n","        \"What was HPS's total revenue in 2022?\",\r\n","        \"What are the strategic priorities of Schneider Electric?\",\r\n","        \"Which company had the highest profitability?\",\r\n","        \"How does Hubble handle sustainability?\"\r\n","    ]\r\n","\r\n","    for q in questions:\r\n","        print(f\"\\n‚ùì Question: {q}\")\r\n","        answer = qa_chain.run(q)\r\n","        print(f\"üí° Answer: {answer}\")\r\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.1"}},"nbformat":4,"nbformat_minor":5}